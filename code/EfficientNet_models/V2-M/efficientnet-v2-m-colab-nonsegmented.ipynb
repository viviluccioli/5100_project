{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsEgp1CTde0m"
      },
      "source": [
        "# EfficientNet-V2-M Hair Type Classifier (CORN Ordinal Regression)\n",
        "\n",
        "Training on YOLO-filtered images with ordinal regression for hair type classification.\n",
        "\n",
        "## Setup\n",
        "1. **Enable GPU**: `Runtime` ‚Üí `Change runtime type` ‚Üí `GPU`\n",
        "2. **Data expected at**: `MyDrive/hair_data/yolo_filtered_serpapi/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqFtes74de0n"
      },
      "source": [
        "## 1. Mount Drive & Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VmqCj8rPde0n",
        "outputId": "37cbdaf1-2ad9-4698-b4bd-b3fc6bf232e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8kOenqNYde0n",
        "outputId": "e7797e85-af4a-41b0-ad3d-8ec0623d61d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU Available: Tesla T4\n",
            "   Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "    raise RuntimeError(\"GPU required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InvU2Fibde0n"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eMxdobwwde0n",
        "outputId": "da382c15-8ed1-4687-84f0-ec795c1bcae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "DATA_DIR = \"/content/drive/MyDrive/hair_data/yolo_filtered_serapi/\"  # Original YOLO images\n",
        "OUTPUT_DIR = \"/content/split/\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/hair_data/checkpoints_corn/\"  # New checkpoint dir\n",
        "\n",
        "# Training parameters\n",
        "IMG_SIZE = 512  # Resize all images to this\n",
        "BATCH_SIZE = 8\n",
        "ACCUMULATION_STEPS = 4  # Effective batch size = 32\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 0.05  # Strong regularization\n",
        "\n",
        "# Data split\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "\n",
        "# Resume?\n",
        "RESUME_FROM_CHECKPOINT = False  # Start fresh with new data\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7rObi3qyde0o",
        "outputId": "f932a3b6-a818-46a9-8982-86a7f3fa7045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Data directory not found: /content/drive/MyDrive/hair_data/yolo_filtered_serpapi/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Data directory not found: /content/drive/MyDrive/hair_data/yolo_filtered_serpapi/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2550630831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå Data directory not found: {DATA_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data directory not found: {DATA_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Data directory not found: /content/drive/MyDrive/hair_data/yolo_filtered_serpapi/"
          ]
        }
      ],
      "source": [
        "# Verify data directory\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"‚ùå Data directory not found: {DATA_DIR}\")\n",
        "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")\n",
        "else:\n",
        "    classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
        "    print(f\"‚úÖ Found {len(classes)} classes: {classes}\")\n",
        "\n",
        "    # Count images per class\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(DATA_DIR, cls)\n",
        "        n_images = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"   {cls}: {n_images} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJlXqJ1Ude0o"
      },
      "source": [
        "## 3. Ordinal Regression (CORN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paZiWWAZde0o"
      },
      "outputs": [],
      "source": [
        "# Hair type ordering (1 ‚Üí 4c is increasing curl)\n",
        "CLASS_ORDER = {\n",
        "    '1': 0, '2a': 1, '2b': 2, '2c': 3,\n",
        "    '3a': 4, '3b': 5, '3c': 6,\n",
        "    '4a': 7, '4b': 8, '4c': 9\n",
        "}\n",
        "\n",
        "class OrdinalClassifier(nn.Module):\n",
        "    \"\"\"Ordinal regression head using CORN method\"\"\"\n",
        "    def __init__(self, in_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.fc = nn.Linear(in_features, num_classes - 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "def corn_loss(logits, labels, num_classes):\n",
        "    \"\"\"CORN loss for ordinal regression\"\"\"\n",
        "    losses = []\n",
        "    for k in range(num_classes - 1):\n",
        "        target = (labels > k).float()\n",
        "        loss_k = F.binary_cross_entropy_with_logits(logits[:, k], target)\n",
        "        losses.append(loss_k)\n",
        "    return torch.stack(losses).mean()\n",
        "\n",
        "\n",
        "def predict_from_ordinal(logits):\n",
        "    \"\"\"Convert ordinal logits to class predictions\"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).sum(dim=1)\n",
        "    return preds\n",
        "\n",
        "print(\"‚úÖ CORN ordinal regression defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6588tIzde0o"
      },
      "source": [
        "## 4. Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkm5u5B3de0o"
      },
      "outputs": [],
      "source": [
        "def split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    \"\"\"Split images into train/val/test\"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(output_dir, split)\n",
        "        if os.path.exists(split_path):\n",
        "            shutil.rmtree(split_path)\n",
        "        os.makedirs(split_path)\n",
        "\n",
        "    classes = sorted([d for d in os.listdir(source_dir)\n",
        "                      if os.path.isdir(os.path.join(source_dir, d))])\n",
        "    print(f\"Found {len(classes)} classes: {classes}\")\n",
        "\n",
        "    stats = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(source_dir, cls)\n",
        "        images = [f for f in os.listdir(cls_path)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if len(images) < 3:\n",
        "            print(f\"‚ö†Ô∏è Skipping '{cls}' - only {len(images)} images\")\n",
        "            continue\n",
        "\n",
        "        train_imgs, temp_imgs = train_test_split(\n",
        "            images, train_size=train_ratio, random_state=42, shuffle=True)\n",
        "        relative_val = val_ratio / (val_ratio + test_ratio)\n",
        "        val_imgs, test_imgs = train_test_split(\n",
        "            temp_imgs, train_size=relative_val, random_state=42, shuffle=True)\n",
        "\n",
        "        for split, img_list in [('train', train_imgs), ('val', val_imgs), ('test', test_imgs)]:\n",
        "            split_cls_path = os.path.join(output_dir, split, cls)\n",
        "            os.makedirs(split_cls_path, exist_ok=True)\n",
        "            for img in img_list:\n",
        "                shutil.copy2(os.path.join(cls_path, img), os.path.join(split_cls_path, img))\n",
        "            stats[cls][split] = len(img_list)\n",
        "\n",
        "    # Print stats\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{'Class':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
        "    print(f\"{'-'*50}\")\n",
        "    total_train, total_val, total_test = 0, 0, 0\n",
        "    for cls in classes:\n",
        "        print(f\"{cls:<10} {stats[cls]['train']:<10} {stats[cls]['val']:<10} {stats[cls]['test']:<10}\")\n",
        "        total_train += stats[cls]['train']\n",
        "        total_val += stats[cls]['val']\n",
        "        total_test += stats[cls]['test']\n",
        "    print(f\"{'-'*50}\")\n",
        "    print(f\"{'TOTAL':<10} {total_train:<10} {total_val:<10} {total_test:<10}\")\n",
        "\n",
        "    return classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWcs7zVPde0o"
      },
      "outputs": [],
      "source": [
        "print(\"=== Splitting Dataset ===\")\n",
        "CLASS_NAMES = split_dataset(DATA_DIR, OUTPUT_DIR, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
        "num_classes = len(CLASS_NAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDfEdeqJde0o"
      },
      "source": [
        "## 5. Data Loaders (with Resizing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc6mrcfHde0o"
      },
      "outputs": [],
      "source": [
        "# Training transforms - resize + augmentation\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize all images to standard size\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Validation transforms - resize only\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize all images to standard size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_ds = datasets.ImageFolder(os.path.join(OUTPUT_DIR, \"train\"), train_tfms)\n",
        "val_ds = datasets.ImageFolder(os.path.join(OUTPUT_DIR, \"val\"), val_tfms)\n",
        "test_ds = datasets.ImageFolder(os.path.join(OUTPUT_DIR, \"test\"), val_tfms)\n",
        "\n",
        "print(f\"\\n‚úÖ Datasets created (resizing to {IMG_SIZE}x{IMG_SIZE})\")\n",
        "print(f\"   Train: {len(train_ds)} images\")\n",
        "print(f\"   Val:   {len(val_ds)} images\")\n",
        "print(f\"   Test:  {len(test_ds)} images\")\n",
        "print(f\"   Classes: {train_ds.classes}\")\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nIAuNQhde0o"
      },
      "source": [
        "## 6. Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQvh_4Dnde0o"
      },
      "outputs": [],
      "source": [
        "# Load pretrained EfficientNet-V2-M\n",
        "print(\"Loading EfficientNet-V2-M...\")\n",
        "weights = EfficientNet_V2_M_Weights.IMAGENET1K_V1\n",
        "model = efficientnet_v2_m(weights=weights)\n",
        "\n",
        "# Get input features and replace classifier with ordinal head\n",
        "in_features = model.classifier[1].in_features\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.5, inplace=True),  # Strong dropout to prevent overfitting\n",
        "    OrdinalClassifier(in_features, num_classes)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n‚úÖ Model ready with CORN ordinal regression\")\n",
        "print(f\"   Parameters: {total_params:,}\")\n",
        "print(f\"   Output: {num_classes - 1} thresholds for {num_classes} ordered classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FUx5vR1de0o"
      },
      "source": [
        "## 7. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr-fSPBUde0o"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch):\n",
        "    \"\"\"Train one epoch with gradient accumulation\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d} Training\", leave=False)\n",
        "\n",
        "    for i, (imgs, labels) in enumerate(pbar):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            logits = model(imgs)\n",
        "            loss = corn_loss(logits, labels, num_classes) / ACCUMULATION_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * ACCUMULATION_STEPS\n",
        "        pbar.set_postfix({'loss': f'{running_loss/(i+1):.4f}'})\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate(loader, desc=\"Validating\"):\n",
        "    \"\"\"Validate with exact and within-1 accuracy\"\"\"\n",
        "    model.eval()\n",
        "    total, correct, correct_w1 = 0, 0, 0\n",
        "    running_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    pbar = tqdm(loader, desc=desc, leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in pbar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                logits = model(imgs)\n",
        "                loss = corn_loss(logits, labels, num_classes)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            preds = predict_from_ordinal(logits)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            correct_w1 += (torch.abs(preds - labels) <= 1).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix({'acc': f'{correct/total:.4f}', '¬±1': f'{correct_w1/total:.4f}'})\n",
        "\n",
        "    accuracy = correct / total\n",
        "    accuracy_w1 = correct_w1 / total\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, accuracy_w1, f1, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ3OE6exde0o"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, history):\n",
        "    \"\"\"Save checkpoint\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'history': history,\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.pth'))\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, scaler, scheduler):\n",
        "    \"\"\"Load checkpoint if exists\"\"\"\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.pth')\n",
        "    if os.path.exists(checkpoint_path) and RESUME_FROM_CHECKPOINT:\n",
        "        print(f\"Found checkpoint at {checkpoint_path}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "            if 'scheduler_state_dict' in checkpoint:\n",
        "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            best_val_acc = checkpoint['best_val_acc']\n",
        "            history = checkpoint['history']\n",
        "            print(f\"‚úÖ Resumed from epoch {checkpoint['epoch']}\")\n",
        "            return start_epoch, best_val_acc, history\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load checkpoint: {e}\")\n",
        "\n",
        "    print(\"Starting fresh training\")\n",
        "    return 1, 0.0, {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'val_acc': [], 'val_acc_within_1': [],\n",
        "        'val_f1': [], 'lr': []\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOSuwdVYde0o"
      },
      "source": [
        "## 8. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b57063BSde0o"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scaler = GradScaler('cuda')\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
        "\n",
        "# Load checkpoint\n",
        "start_epoch, best_val_acc, history = load_checkpoint(model, optimizer, scaler, scheduler)\n",
        "\n",
        "# Early stopping\n",
        "patience = 5\n",
        "epochs_no_improve = 0\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training Configuration\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: EfficientNet-V2-M + CORN\")\n",
        "print(f\"Data: YOLO-filtered images (resized to {IMG_SIZE}x{IMG_SIZE})\")\n",
        "print(f\"Epochs: {start_epoch} to {EPOCHS}\")\n",
        "print(f\"Batch: {BATCH_SIZE} (effective: {BATCH_SIZE * ACCUMULATION_STEPS})\")\n",
        "print(f\"LR: {LR} (cosine annealing)\")\n",
        "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
        "print(f\"Early stopping patience: {patience}\")\n",
        "print(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amyJzecNde0o"
      },
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch, EPOCHS + 1):\n",
        "    start = time.time()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Train & validate\n",
        "    train_loss = train_one_epoch(epoch)\n",
        "    val_loss, val_acc, val_acc_w1, val_f1, _, _ = validate(val_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "    duration = time.time() - start\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_acc_within_1'].append(val_acc_w1)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
        "          f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
        "          f\"Acc: {val_acc:.4f} | \"\n",
        "          f\"¬±1: {val_acc_w1:.4f} | \"\n",
        "          f\"F1: {val_f1:.4f} | \"\n",
        "          f\"LR: {current_lr:.2e} | \"\n",
        "          f\"{duration:.0f}s\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, \"best_model.pth\"))\n",
        "        print(f\"  ‚úÖ New best! (acc: {best_val_acc:.4f}, ¬±1: {val_acc_w1:.4f})\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"  üìâ No improvement ({epochs_no_improve}/{patience})\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, scaler, scheduler, best_val_acc, history)\n",
        "\n",
        "    # Early stopping\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Training Complete!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpnz8fHade0o"
      },
      "source": [
        "## 9. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCCA2HsFde0o"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "best_path = os.path.join(CHECKPOINT_DIR, \"best_model.pth\")\n",
        "if os.path.exists(best_path):\n",
        "    model.load_state_dict(torch.load(best_path, weights_only=True))\n",
        "    print(f\"Loaded best model from {best_path}\")\n",
        "\n",
        "test_loss, test_acc, test_acc_w1, test_f1, test_preds, test_labels = validate(test_loader, \"Testing\")\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"TEST RESULTS\")\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"Exact Accuracy:    {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
        "print(f\"Within-1 Accuracy: {test_acc_w1:.4f} ({test_acc_w1*100:.1f}%)\")\n",
        "print(f\"F1 Score:          {test_f1:.4f}\")\n",
        "print(f\"{'='*40}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG-dRNkCde0p"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(test_labels, test_preds, target_names=CLASS_NAMES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6PuZ7ixde0p"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Hair Type Classification (CORN)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CHECKPOINT_DIR, 'confusion_matrix.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2E3gtLQde0p"
      },
      "source": [
        "## 10. Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mSYL-gWde0p"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(epochs_range, history['train_loss'], label='Train', marker='o')\n",
        "axes[0, 0].plot(epochs_range, history['val_loss'], label='Val', marker='o')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 1].plot(epochs_range, history['val_acc'], label='Exact', marker='o', color='green')\n",
        "axes[0, 1].plot(epochs_range, history['val_acc_within_1'], label='Within-1', marker='o', color='blue')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Validation Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# F1\n",
        "axes[1, 0].plot(epochs_range, history['val_f1'], label='F1', marker='o', color='orange')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('F1 Score')\n",
        "axes[1, 0].set_title('Validation F1')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# LR\n",
        "axes[1, 1].plot(epochs_range, history['lr'], label='LR', marker='o', color='red')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Learning Rate')\n",
        "axes[1, 1].set_title('Learning Rate')\n",
        "axes[1, 1].set_yscale('log')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_history.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beyLKUuIde0p"
      },
      "source": [
        "## 11. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpz-9IF-de0p"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def predict_hair_type(image_path, top_k=3):\n",
        "    \"\"\"Predict hair type with confidence scores\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and transform\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(img_tensor)\n",
        "        pred = predict_from_ordinal(logits).item()\n",
        "        probs = torch.sigmoid(logits)[0].cpu().numpy()\n",
        "\n",
        "    pred_class = CLASS_NAMES[pred]\n",
        "\n",
        "    # Get neighboring classes too\n",
        "    neighbors = []\n",
        "    for offset in [-1, 0, 1]:\n",
        "        idx = pred + offset\n",
        "        if 0 <= idx < len(CLASS_NAMES):\n",
        "            neighbors.append(CLASS_NAMES[idx])\n",
        "\n",
        "    return pred_class, neighbors\n",
        "\n",
        "# Example:\n",
        "# pred, neighbors = predict_hair_type(\"/path/to/image.jpg\")\n",
        "# print(f\"Predicted: {pred}\")\n",
        "# print(f\"Likely range: {neighbors}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}